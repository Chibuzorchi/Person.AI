#!/usr/bin/env python3
"""
AI Quality Validation Baselines Demo
Demonstrates baseline management and threshold testing
"""

import json
import os
import shutil
from pathlib import Path
from typing import Dict, Any
from ai_quality_validation import AIQualityValidator

def demo_baseline_deletion():
    """Demo: What happens when baseline files are deleted"""
    print("🎯 DEMO 1: Baseline File Deletion")
    print("=" * 50)
    
    # Create a backup of original baselines
    baselines_dir = Path("ai_quality_baselines")
    backup_dir = Path("ai_quality_baselines_backup")
    
    if backup_dir.exists():
        shutil.rmtree(backup_dir)
    shutil.copytree(baselines_dir, backup_dir)
    print("✅ Created backup of original baselines")
    
    # Show current baselines
    print(f"\n📁 Current baselines:")
    for file in baselines_dir.glob("*.json"):
        print(f"  - {file.name}")
    
    # Delete a baseline file
    text_baseline = baselines_dir / "text_baseline.json"
    if text_baseline.exists():
        text_baseline.unlink()
        print(f"\n🗑️  DELETED: {text_baseline.name}")
    
    # Test with validator (should auto-create new baseline)
    print("\n🧪 Testing with deleted baseline...")
    validator = AIQualityValidator()
    
    test_content = "Daily Brief - 2024-01-15\n\nEmail Summary:\n• New client inquiry - contact@newclient.com\n• Project update - manager@company.com\n\nKey Insights:\n• 2 new emails received\n• 1 client inquiry requires response\n• 1 project update to review\n\nAction Items:\n• Respond to client inquiry\n• Review project update\n\nGenerated by Person.ai"
    
    result = validator.validate_content_quality(test_content, "text")
    
    print(f"✅ Result: {result['validations']['semantic_similarity']['passed']}")
    print(f"📊 Similarity Score: {result['validations']['semantic_similarity']['score']:.3f}")
    print(f"📝 Details: {result['validations']['semantic_similarity']['details']}")
    
    # Check if new baseline was created
    if text_baseline.exists():
        print(f"\n🆕 NEW BASELINE CREATED: {text_baseline.name}")
        with open(text_baseline, 'r') as f:
            new_baseline = json.load(f)
        print(f"   Content length: {new_baseline['content_length']} chars")
        print(f"   Word count: {new_baseline['word_count']} words")
        print(f"   Created at: {new_baseline['created_at']}")
    
    # Restore original baselines
    shutil.rmtree(baselines_dir)
    shutil.move(backup_dir, baselines_dir)
    print(f"\n🔄 Restored original baselines")

def demo_threshold_adjustment():
    """Demo: What happens when similarity threshold is changed"""
    print("\n\n🎯 DEMO 2: Similarity Threshold Adjustment")
    print("=" * 50)
    
    validator = AIQualityValidator()
    
    # Test content that's similar but not identical
    test_content = "Daily Brief - 2024-01-15\n\nEmail Summary:\n• Quarterly Sales Report - john@company.com\n• Client Meeting Request - sarah@client.com\n• Invoice Payment Overdue - billing@vendor.com\n\nKey Insights:\n• 3 urgent items require immediate attention\n• 2 client meetings scheduled for this week\n• 1 invoice payment overdue\n• New project proposal received\n\nAction Items:\n• Review quarterly sales report\n• Schedule follow-up meeting with client\n• Process invoice payment\n• Prepare project proposal\n\nGenerated by Person.ai"
    
    print("📝 Test content (similar to baseline):")
    print(f"   Length: {len(test_content)} chars")
    print(f"   Preview: {test_content[:100]}...")
    
    # Test with default threshold (0.8)
    print(f"\n🧪 Testing with DEFAULT threshold (0.8)...")
    result_default = validator.validate_content_quality(test_content, "text")
    default_score = result_default['validations']['semantic_similarity']['score']
    default_passed = result_default['validations']['semantic_similarity']['passed']
    
    print(f"   Score: {default_score:.3f}")
    print(f"   Passed: {'✅ YES' if default_passed else '❌ NO'}")
    
    # Modify threshold to 0.95 (higher = more strict)
    print(f"\n🔧 Modifying threshold to 0.95 (more strict)...")
    
    # Create a custom validator with higher threshold
    class StrictValidator(AIQualityValidator):
        def _validate_semantic_similarity(self, content: str, content_type: str) -> Dict[str, Any]:
            baseline_key = f"{content_type}_baseline"
            if baseline_key not in self.baselines:
                return {"score": 0.0, "passed": False, "details": "No baseline available"}
            
            baseline = self.baselines[baseline_key]
            similarity = self._calculate_similarity(content, baseline["content"])
            
            threshold = 0.95  # Higher threshold
            passed = similarity >= threshold
            
            return {
                "score": similarity,
                "passed": passed,
                "details": f"Similarity: {similarity:.3f} (threshold: {threshold})",
                "similarity": similarity,
                "baseline_created": baseline["created_at"]
            }
    
    strict_validator = StrictValidator()
    result_strict = strict_validator.validate_content_quality(test_content, "text")
    strict_score = result_strict['validations']['semantic_similarity']['score']
    strict_passed = result_strict['validations']['semantic_similarity']['passed']
    
    print(f"   Score: {strict_score:.3f}")
    print(f"   Passed: {'✅ YES' if strict_passed else '❌ NO'}")
    
    # Show the difference
    print(f"\n📊 COMPARISON:")
    print(f"   Default (0.8):  {default_score:.3f} - {'✅ PASS' if default_passed else '❌ FAIL'}")
    print(f"   Strict (0.95):  {strict_score:.3f} - {'✅ PASS' if strict_passed else '❌ FAIL'}")
    
    if default_passed and not strict_passed:
        print(f"\n🎯 DEMO SUCCESS: Same content passes at 0.8 but fails at 0.95!")
    elif default_passed and strict_passed:
        print(f"\n📝 Note: Content is similar enough to pass both thresholds")
    else:
        print(f"\n📝 Note: Content failed both thresholds")

def demo_baseline_corruption():
    """Demo: What happens when baseline content is modified"""
    print("\n\n🎯 DEMO 3: Baseline Content Modification")
    print("=" * 50)
    
    baselines_dir = Path("ai_quality_baselines")
    text_baseline = baselines_dir / "text_baseline.json"
    
    # Backup original
    backup_file = text_baseline.with_suffix('.json.backup')
    shutil.copy2(text_baseline, backup_file)
    
    # Modify baseline content
    with open(text_baseline, 'r') as f:
        baseline_data = json.load(f)
    
    original_content = baseline_data["content"]
    baseline_data["content"] = "COMPLETELY DIFFERENT CONTENT - NOT SIMILAR AT ALL"
    
    with open(text_baseline, 'w') as f:
        json.dump(baseline_data, f, indent=2)
    
    print("🔧 Modified baseline content to be completely different")
    
    # Test with validator
    validator = AIQualityValidator()
    test_content = "Daily Brief - 2024-01-15\n\nEmail Summary:\n• Quarterly Sales Report - john@company.com\n• Client Meeting Request - sarah@client.com\n\nKey Insights:\n• 2 urgent items require attention\n• 1 client meeting scheduled\n\nAction Items:\n• Review sales report\n• Schedule client meeting\n\nGenerated by Person.ai"
    
    result = validator.validate_content_quality(test_content, "text")
    score = result['validations']['semantic_similarity']['score']
    passed = result['validations']['semantic_similarity']['passed']
    
    print(f"\n🧪 Testing with corrupted baseline...")
    print(f"   Score: {score:.3f}")
    print(f"   Passed: {'✅ YES' if passed else '❌ NO'}")
    print(f"   Details: {result['validations']['semantic_similarity']['details']}")
    
    # Restore original
    shutil.copy2(backup_file, text_baseline)
    backup_file.unlink()
    print(f"\n🔄 Restored original baseline")

def main():
    """Run all demos"""
    print("🚀 AI Quality Validation Baselines Demo")
    print("=" * 60)
    print("This demo shows how the AI quality validation system handles:")
    print("1. Deleted baseline files (auto-creation)")
    print("2. Threshold adjustments (0.8 → 0.95)")
    print("3. Baseline content corruption")
    print("=" * 60)
    
    try:
        demo_baseline_deletion()
        demo_threshold_adjustment()
        demo_baseline_corruption()
        
        print("\n\n🎉 ALL DEMOS COMPLETED!")
        print("=" * 60)
        print("Key takeaways:")
        print("• Deleted baselines → auto-created (test 'passes' but resets baseline)")
        print("• Higher thresholds → stricter validation (0.8 → 0.95)")
        print("• Corrupted baselines → low similarity scores")
        print("• System is robust and handles edge cases gracefully")
        
    except Exception as e:
        print(f"\n❌ Demo failed: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
