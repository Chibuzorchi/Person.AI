# üéØ Tiered Testing Strategy for 150+ Integrations

## Overview

This document outlines our scalable testing approach for Person.ai's growing integration ecosystem. As we scale from 5 to 150+ integrations, we need a strategy that keeps feedback fast while maintaining comprehensive coverage.

## üèóÔ∏è Testing Tiers

### Tier 1: Critical Integrations (Every Commit)
**Run Time**: ~10 minutes  
**Frequency**: Every commit, PR, and deployment  
**Integrations**: Slack, Gmail, Salesforce, E2E Pipeline

```bash
# Run Tier 1 tests
pytest -m tier1_critical -v

# Run specific component
cd slack-mock && pytest -m tier1_critical -v
```

**Why These**: Core to Person.ai's value proposition. Any failure here blocks deployment.

### Tier 2: Important Integrations (Schema Changes)
**Run Time**: ~15 minutes  
**Frequency**: On API schema changes, nightly builds  
**Integrations**: QuickBooks, Stripe, HubSpot, Zendesk

```bash
# Run Tier 2 tests
pytest -m tier2_important -v

# Run on schema changes
git commit -m "Update QuickBooks API schema"  # Triggers Tier 2
```

**Why These**: Important for business operations but not blocking for every change.

### Tier 3: Secondary Integrations (Weekly)
**Run Time**: ~20 minutes  
**Frequency**: Weekly, on-demand, or when adding new integrations  
**Integrations**: Notion, Dropbox, Trello, Asana, Monday.com

```bash
# Run Tier 3 tests
pytest -m tier3_secondary -v

# Run weekly (automated)
# Runs every Monday at 2 AM via GitHub Actions
```

**Why These**: Nice-to-have integrations that don't need constant validation.

## üöÄ Parallel Execution Strategy

### Current Setup (5 Integrations)
```bash
# Sequential execution
pytest slack-mock/ test-data-seeding/ e2e-testing/ monitoring-system/ bubble-frontend-mock/

# Parallel execution (3x faster)
python run_tiered_tests.py --tier all --parallel
```

### Scaling to 150+ Integrations
```bash
# Tier 1: 20 critical integrations in parallel
pytest -m tier1_critical -n 20  # 20 parallel workers

# Tier 2: 50 important integrations in parallel  
pytest -m tier2_important -n 50  # 50 parallel workers

# Tier 3: 80+ secondary integrations in parallel
pytest -m tier3_secondary -n 80  # 80 parallel workers
```

## üìä Test Execution Commands

### Quick Commands
```bash
# Run only critical tests (fast feedback)
python run_tiered_tests.py --tier 1

# Run important tests (schema changes)
python run_tiered_tests.py --tier 2

# Run secondary tests (weekly)
python run_tiered_tests.py --tier 3

# Run smoke tests (PR validation)
python run_tiered_tests.py --tier smoke

# Run full regression suite
python run_tiered_tests.py --tier regression
```

### Advanced Commands
```bash
# Run specific integration tier
pytest -m "tier1_critical and integration" -v

# Run standalone tests only
pytest -m "tier2_important and standalone" -v

# Run with parallel execution
pytest -m tier1_critical -n 10 --dist=loadfile

# Run with coverage
pytest -m tier1_critical --cov=slack_mock --cov-report=html
```

## üîÑ CI/CD Integration

### GitHub Actions Triggers
```yaml
# Every commit
on: [push, pull_request]
  - tier1-critical: Always runs
  - smoke-tests: PR validation

# Schema changes  
on: push
  if: contains(message, 'schema') || contains(message, 'api')
  - tier2-important: Runs automatically

# Weekly schedule
on: schedule
  - tier3-secondary: Runs every Monday
  - parallel-execution: Full regression suite
```

### Commit Message Triggers
```bash
# Trigger Tier 2 tests
git commit -m "Update Salesforce API schema"

# Trigger full regression
git commit -m "Add new integration - run full-test"

# Trigger parallel execution
git commit -m "Performance testing - run parallel"
```

## üìà Scaling Benefits

### Time Savings
- **Sequential**: 150 integrations √ó 2min = 5 hours
- **Parallel**: 150 integrations √∑ 20 workers = 15 minutes
- **Tiered**: Only run what's needed (10-20 minutes typical)

### Resource Optimization
- **Tier 1**: High priority, fast feedback
- **Tier 2**: Medium priority, scheduled runs
- **Tier 3**: Low priority, batch processing

### Cost Efficiency
- **CI Minutes**: 90% reduction in compute time
- **Developer Productivity**: Faster feedback loops
- **Maintenance**: Easier to add new integrations

## üõ†Ô∏è Adding New Integrations

### Step 1: Categorize Integration
```python
# Critical (Tier 1)
pytestmark = [pytest.mark.tier1_critical, pytest.mark.integration]

# Important (Tier 2)  
pytestmark = [pytest.mark.tier2_important, pytest.mark.integration]

# Secondary (Tier 3)
pytestmark = [pytest.mark.tier3_secondary, pytest.mark.integration]
```

### Step 2: Add to Test Runner
```python
# In run_tiered_tests.py
def run_tier1_critical(self):
    commands = [
        ("pytest -m 'tier1_critical and standalone' -v", "slack-mock", "tier1"),
        ("pytest -m 'tier1_critical and standalone' -v", "new-integration", "tier1"),  # Add here
    ]
```

### Step 3: Update CI/CD
```yaml
# In .github/workflows/tiered-testing.yml
strategy:
  matrix:
    component: [slack-mock, e2e-testing, new-integration]  # Add here
```

## üìã Test Markers Reference

| Marker | Description | Usage |
|--------|-------------|-------|
| `tier1_critical` | Critical integrations | Every commit |
| `tier2_important` | Important integrations | Schema changes |
| `tier3_secondary` | Secondary integrations | Weekly |
| `standalone` | No external dependencies | Fast execution |
| `integration` | Requires Docker services | Full validation |
| `smoke` | Quick validation | PR checks |
| `regression` | Full test suite | Release validation |
| `performance` | Load testing | Performance validation |

## üéØ Best Practices

### 1. Test Design
- **Fast**: Tier 1 tests should complete in <10 minutes
- **Reliable**: Use mocks for external dependencies
- **Isolated**: Tests shouldn't depend on each other
- **Parallel**: Design tests to run concurrently

### 2. CI/CD Strategy
- **Fail Fast**: Stop on Tier 1 failures
- **Smart Triggers**: Use commit messages to control test runs
- **Resource Limits**: Set timeouts to prevent hanging
- **Artifact Collection**: Save test results for debugging

### 3. Monitoring
- **Success Rates**: Track test reliability over time
- **Execution Times**: Monitor performance trends
- **Flaky Tests**: Identify and fix unstable tests
- **Coverage**: Ensure adequate test coverage

## üöÄ Future Enhancements

### Phase 1: Current (5 integrations)
- ‚úÖ Tiered testing framework
- ‚úÖ Parallel execution
- ‚úÖ CI/CD integration

### Phase 2: Scaling (50 integrations)
- üîÑ Dynamic test selection
- üîÑ Test result caching
- üîÑ Smart retry logic

### Phase 3: Enterprise (150+ integrations)
- üîÑ Machine learning test prioritization
- üîÑ Distributed test execution
- üîÑ Real-time test analytics

---

**This strategy ensures Person.ai can scale to 150+ integrations while maintaining fast, reliable, and cost-effective testing!** üéâ
